{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un'introduzione operativa agli LLM\n",
    "### Con accesso a un LLM locale attraverso un server di API\n",
    "\n",
    "Luca Mari, febbraio 2024\n",
    "\n",
    "[i file di questa attività: [base.ipynb](base.ipynb), [baseutils.py](baseutils.py), [base.py](base.py)]\n",
    "\n",
    "**Obiettivi**: comprendere la logica dell'accesso a un LLM attraverso una API.  \n",
    "**Precompetenze**: basi di Python.\n",
    "embedding_layer.embedding_dim\n",
    "Al momento un sistema semplice per eseguire sul proprio computer un LLM e accedere a esso attraverso una API è LM Studio:\n",
    "* scaricare da https://lmstudio.ai e installare LM Studio\n",
    "* eseguire LM Studio e seguendo le indicazioni nel programma:\n",
    "    * scaricare dalla rete un LLM (per esempio **TheBloke mistral openorca 7B Q4_0 gguf**)\n",
    "    * caricare il LLM\n",
    "    * mettere in esecuzione il server\n",
    "\n",
    "Occorre ora creare un ambiente di lavoro Python, supponiamo con VSCode:\n",
    "* installare un interprete Python\n",
    "* scaricare da https://code.visualstudio.com/download e installare VSCode\n",
    "* eseguire VSCode e attivare le estensioni per Python e Jupyter\n",
    "* ancora in VSCode:\n",
    "    * creare una cartella di lavoro e renderla la cartella corrente:  \n",
    "    * copiare nella cartella questo notebook e il file `baseutils.py` e aprire il notebook\n",
    "    * creare un ambiente virtuale locale Python (Select Kernel | Python Environments | Create Python Environment | Venv, e scegliere un interprete Python):  \n",
    "    * installare il modulo Python richiesto, eseguendo dal terminale:  \n",
    "        `pip install openai`\n",
    "\n",
    "Eseguendo l'applicazione Python `base.py` si attiva poi una semplice chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa, importiamo il modulo Python che contiene le funzioni per consentire un accesso \"di alto livello\" al LLM in esecuzione in LM Studio e quindi attiviamo la connessione al LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseutils import llm\n",
    "llm = llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto siamo già pronti per fare una domanda al LLM, ricevere e visualizzare la risposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-jd65f2zin1wviybz0hy9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The Bayesian interpretation of probability defines it as a degree of belief or certainty in an event's occurrence, which can be updated with new evidence. It contrasts with frequentist probability, which is based on the long-run frequency of events in repeated trials.\", role='assistant', function_call=None, tool_calls=None))], created=1710409587, model='/home/lucamari/.local/share/nomic.ai/GPT4All/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/mixtral-8x7b-instruct-v0.1.Q8_0.gguf', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=55, prompt_tokens=272, total_tokens=327))\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are the main features of the Bayesian interpretation of probability? Please answer in one or two sentences.\"\n",
    "answer = llm.request(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La risposta non è molto leggibile, perché è un oggetto Python. Per curiosità, ne possiamo visualizzare il contenuto come un oggetto JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-jd65f2zin1wviybz0hy9\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"The Bayesian interpretation of probability defines it as a degree of belief or certainty in an event's occurrence, which can be updated with new evidence. It contrasts with frequentist probability, which is based on the long-run frequency of events in repeated trials.\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1710409587,\n",
      "  \"model\": \"/home/lucamari/.local/share/nomic.ai/GPT4All/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/mixtral-8x7b-instruct-v0.1.Q8_0.gguf\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 55,\n",
      "    \"prompt_tokens\": 272,\n",
      "    \"total_tokens\": 327\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from baseutils import jprint\n",
    "jprint(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per visualizzare solo il testo della risposta, usiamo la funzione `bprint` dal modulo `utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bayesian interpretation of probability defines it as a degree of belief or certainty in an\n",
      "event's occurrence, which can be updated with new evidence. It contrasts with frequentist\n",
      "probability, which is based on the long-run frequency of events in repeated trials.\n"
     ]
    }
   ],
   "source": [
    "from baseutils import bprint\n",
    "bprint(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ancora meglio, possiamo attivare la funzione per visualizzare i token progressivamente, mano a mano che vengono generati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bayesian interpretation of probability posits that probability is a measure of belief or confidence\n",
      " in the occurrence of an event, which can be updated as new evidence becomes available through the application\n",
      " of Bayes' theorem. It contrasts with frequentist interpretations that define probability based on long\n",
      "-run frequencies in repeated trials."
     ]
    }
   ],
   "source": [
    "from baseutils import aprint\n",
    "aprint(llm.request(prompt, stream=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
