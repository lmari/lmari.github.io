{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un'esplorazione del word embedding\n",
    "\n",
    "Luca Mari, marzo 2024\n",
    "\n",
    "[i file di questa attività: [embed.ipynb](embed.ipynb), [embedutils.py](embedutils.py), [embedexpl.py](embedexpl.py), [embedtempl.html](embedtempl.html)]\n",
    "\n",
    "**Obiettivi**: comprendere la logica del \"word embedding\", il processo con cui una rete neurale artificiale converte espressioni linguistiche semplici (parole appunto, e più correttamente token, da un vocabolario) nei vettori numerici su cui i transformer poi operano.  \n",
    "**Precompetenze**: basi di Python.\n",
    "\n",
    "Occorre creare un ambiente di lavoro Python, supponiamo con VSCode:\n",
    "* installare un interprete Python\n",
    "* scaricare da https://code.visualstudio.com/download e installare VSCode\n",
    "* eseguire VSCode e attivare le estensioni per Python e Jupyter\n",
    "* ancora in VSCode:\n",
    "    * creare una cartella di lavoro e renderla la cartella corrente:  \n",
    "    * copiare nella cartella questo notebook e il file `embedutils.py` e aprire il notebook\n",
    "    * creare un ambiente virtuale locale Python (Select Kernel | Python Environments | Create Python Environment | Venv, e scegliere un interprete Python):  \n",
    "    * installare i moduli Python richiesti, eseguendo dal terminale:  \n",
    "        `pip install torch transformers multimethod`\n",
    "\n",
    "Lo spazio di word embedding può essere esplorato anche interattivamente, mediante un'applicazione web. Per eseguire il visualizzatore interattivo web occorre:\n",
    "* aver eseguito almeno una volta questo notebook, in modo da aver scaricato il transformer `BERT`\n",
    "* copiare nella stessa cartella con questo notebook e `embedutils.py` anche i file `embedexpl.py` e `embedtempl.html`\n",
    "* installare altri moduli Python, ancora eseguendo dal terminale:  \n",
    "    `pip install flask networkx`\n",
    "* eseguire il visualizzatore `embedexpl.py`\n",
    "* una volta avviata l'applicazione, aprire un browser e digitare l'indirizzo:  \n",
    "    `http://127.0.0.1:5000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa, importiamo il modulo che contiene le funzioni per consentire un accesso \"di alto livello\" al modello pre-addestrato che opererà sia come tokenizzatore sia come sistema di embedding, usando in questo caso una versione pre-addestrata e fine tuned, su testi in italiano, di `BERT`, che è un transformer \"open\" (https://it.wikipedia.org/wiki/BERT) ed eseguibile anche localmente (alla prima esecuzione sarà dunque necessario attendere che il modello sia scaricato dal sito di Hugging Face: è un file di circa 400 MB che viene copiato nella cartella HFdata della propria cartella personale) (non discutiamo qui di come questo modello sia stato addestrato a fare embedding).\n",
    "\n",
    "Dopo aver caricato il modello, verifichiamo che il processo sia andato a buon fine visualizzando le due informazioni principali:\n",
    "* il numero di token riconosciuti nel vocabolario del modello (`model.vocab_size`);\n",
    "* la dimensione del vettore in cui ogni token viene embedded (`model.embedding_dim`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il tokenizzatore ha un vocabolario di 31102 token che riconosce.\n",
      "Ogni token viene mappato ('embedded') in un vettore di 768 numeri.\n",
      "La matrice degli embeddings ha perciò dimensione (32102, 768)\n"
     ]
    }
   ],
   "source": [
    "from embedutils import Model\n",
    "from pprint import pprint\n",
    "\n",
    "model = Model('dbmdz/bert-base-italian-xxl-cased', True)\n",
    "\n",
    "print(f\"Il tokenizzatore ha un vocabolario di {model.vocab_size} token che riconosce.\")\n",
    "print(f\"Ogni token viene mappato ('embedded') in un vettore di {model.embedding_dim} numeri.\")\n",
    "print(f\"La matrice degli embeddings ha perciò dimensione {model.vocab_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il tokenizzatore mantiene un vocabolario dei token che riconosce, in una tabella in cui a ogni token è associato un identificatore univoco (`id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dato un token, come 'bellezza', il tokenizzatore è in grado di trovarne l'identificatore: 6108\n",
      "(ai token non presenti nel vocabolario è associato l'identificatore 101).\n"
     ]
    }
   ],
   "source": [
    "token = \"bellezza\"\n",
    "token_id = model.token_to_id(token)\n",
    "print(f\"Dato un token, come '{token}', il tokenizzatore è in grado di trovarne l'identificatore: {token_id}\")\n",
    "print(f\"(ai token non presenti nel vocabolario è associato l'identificatore {model.tokenizer.convert_tokens_to_ids(model.tokenizer.unk_token)}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello è stato addestrato a mappare ('embed') ogni token, con il suo identificatore, in un vettore di numeri (c'è da considerare che i transformer, come `BERT`, operano sulla base di un embedding dinamico, in cui il vettore di numeri associato a ogni token dipende anche dal contesto ('embedding posizionale'): qui noi lavoriamo solo con la componente statica del vettore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il token 'bellezza' è mappato in un vettore i cui primi 10 elementi sono [ 0.03051873  0.01173639 -0.04997671  0.0277972   0.02349026  0.00617846\n",
      " -0.04041     0.06573588 -0.02823343  0.02561735]\n"
     ]
    }
   ],
   "source": [
    "embedding = model.token_to_embedding(token)\n",
    "print(f\"Il token '{token}' è mappato in un vettore i cui primi 10 elementi sono {embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'embedding è dunque una funzione dall'insieme dei token riconosciuti, cioè il vocabolario, allo spazio metrico dei vettori a *n* dimensioni. Il modello che realizza tale funzione è addestrato in modo tale da cercare di indurre una struttura metrica sul vocabolario, sulla base del principio che token di significato simile dovrebbero essere associati a vettori vicini. Dato un token, è così possibile elencare i token che gli sono più simili nel vocabolario, cioè quelli che associati a vettori più vicini al vettore del token dato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bellezze', 0.45),\n",
      " ('dolcezza', 0.38),\n",
      " ('splendore', 0.38),\n",
      " ('estetica', 0.35),\n",
      " ('fascino', 0.34)]\n"
     ]
    }
   ],
   "source": [
    "pprint(model.most_similar(token, top_n=5, filter=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'embedding consente di operare in modo piuttosto sofisticato sul vocabolario.  \n",
    "Per esempio, dati due termini A e B a ognuno dei quali è stato associato un vettore, v(A) e v(B), se la regola di associazione v(.) è sufficientemente ricca da un punto di vista semantico allora il vettore differenza v(A)-v(B) è associato alla relazione tra A e B. In questo modo diventa possibile operare con relazioni di \"proporzionalità semantica\", del tipo: data la relazione tra \"re\" e \"uomo\", qual è il termine X che è nella stessa relazione con \"donna\"? Questa domanda è dunque codificata come v(\"re\")-v(\"uomo)=v(X)-v(\"donna\"), e perciò v(X)=v(\"re\")+v(\"donna\")-v(\"uomo), in cui \"re\" e \"donna\" sono gli \"esempi positivi\" e \"uomo\" è l'\"esempio negativo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('regina', 0.31)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"re\", \"donna\"]             # sovrano donna\n",
    "negative_examples = [\"uomo\"]\n",
    "print(model.most_similar(positive_examples, negative_examples, top_n=1, filter=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alcuni altri esempi analoghi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Parigi', 0.48)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"Roma\", \"Francia\"]        # capitale della Francia\n",
    "negative_examples = [\"Italia\"]\n",
    "print(model.most_similar(positive_examples, negative_examples, top_n=1, filter=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Spagna', 0.39)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"Italia\", \"Catalogna\"]     # stato a cui appartiene la Catalogna\n",
    "negative_examples = [\"Lombardia\"]\n",
    "print(model.most_similar(positive_examples, negative_examples, top_n=1, filter=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('madre', 0.56)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"padre\", \"figlia\"]         # genitore femmina\n",
    "negative_examples = [\"figlio\"]\n",
    "print(model.most_similar(positive_examples, negative_examples, top_n=1, filter=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('inverno', 0.5)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"estate\", \"freddo\"]        # stagione fredda\n",
    "negative_examples = [\"caldo\"]\n",
    "print(model.most_similar(positive_examples, negative_examples, top_n=1, filter=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nera', 0.56)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"bianca\", \"nero\"]          # femminile di \"nero\"\n",
    "negative_examples = [\"bianco\"]\n",
    "print(model.most_similar(positive_examples, negative_examples, top_n=1, filter=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('automobile', 0.54)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"treno\", \"automobili\"]     # singolare di \"automobili\"\n",
    "negative_examples = [\"treni\"]\n",
    "print(model.most_similar(positive_examples, negative_examples, top_n=1, filter=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('guardare', 0.57)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"andare\", \"guardato\"]      # infinito di \"guardato\"\n",
    "negative_examples = [\"andato\"]\n",
    "print(model.most_similar(positive_examples, negative_examples, top_n=1, filter=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('buono', 0.37)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"bello\", \"cattivo\"]        # opposto di \"cattivo\"\n",
    "negative_examples = [\"brutto\"]\n",
    "print(model.most_similar(positive_examples, negative_examples, top_n=1, filter=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pianoforte', 0.63)]\n"
     ]
    }
   ],
   "source": [
    "positive_examples = [\"chitarra\", \"pianista\"]    # strumento del pianista\n",
    "negative_examples = [\"chitarrista\"]\n",
    "print(model.most_similar(positive_examples, negative_examples, top_n=1, filter=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
