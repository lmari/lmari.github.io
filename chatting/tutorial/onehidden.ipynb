{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'approssimazione di una funzione mediante un MLP con uno strato interno\n",
    "\n",
    "Luca Mari, novembre 2024  \n",
    "\n",
    "Quest'opera è distribuita con <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0\" target=\"_blank\">Licenza Creative Commons Attribuzione - Non commerciale - Condividi allo stesso modo 4.0 Internazionale</a>.  \n",
    "<img src=\"https://creativecommons.it/chapterIT/wp-content/uploads/2021/01/by-nc-sa.eu_.png\" width=\"100\">\n",
    "\n",
    "**Obiettivo**: comprendere, a partire da un esempio concreto, che una rete neurale deve includere degli elementi non lineari per poter approssimare appropriatamente anche delle semplici funzioni non lineari.  \n",
    "**Precompetenze**: basi di Python; almeno qualche idea di analisi matematica.\n",
    "\n",
    "> Per eseguire questo notebook, supponiamo con VSCode, occorre:\n",
    "> * installare un interprete Python\n",
    "> * scaricare da https://code.visualstudio.com/download e installare VSCode\n",
    "> * eseguire VSCode e attivare le estensioni per Python e Jupyter\n",
    "> * ancora in VSCode:\n",
    ">     * creare una cartella di lavoro e renderla la cartella corrente\n",
    ">     * copiare nella cartella il file di questa attività: [onehidden.ipynb](onehidden.ipynb)\n",
    ">     * aprire il notebook `onehidden.ipynb`\n",
    ">     * creare un ambiente virtuale locale Python (Select Kernel | Python Environments | Create Python Environment | Venv, e scegliere un interprete Python):\n",
    ">     * installare il modulo Python richiesto, eseguendo dal terminale:  \n",
    ">         `pip install torch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo già considerato che una rete neurale è l'implementazione di una funzione parametrica $Y = f(X; \\theta)$, e può essere intesa come uno strumento di approssimazione di funzioni $F(X)$ date: attraverso un opportuno addestramento, si trovano i valori appropriati dei parametri $\\theta$ in modo che $f(X; \\theta) \\approx F(X)$. E abbiamo sperimentato che una rete costituita da un solo neurone a comportamento lineare non è in grado di approssimare in modo accettabile anche funzioni molto semplici.\n",
    "\n",
    "Continuando ad assumere di voler approssimare funzioni $F: \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}$, rendiamo allora la rete un poco più complessa, introducendo tra i due input e l'output uno o più _nodi_ (d'ora in poi chiameremo così i neuroni) di uno _strato interno_ (_hidden layer_), con la condizione che la rete sia _fully connected_:  \n",
    "-- tutti gli input sono connessi a tutti i nodi dello strato interno, e  \n",
    "-- tutti i nodi dello strato interno sono connessi al nodo di output.  \n",
    "Se per esempio lo strato interno ha due nodi, la struttura della rete è dunque:\n",
    "\n",
    "![rete](onehidden.drawio.svg)\n",
    "\n",
    "Per costruire e operare sulla rete useremo `PyTorch`: importiamo perciò i moduli Python che saranno necessari e verifichiamo se è disponibile una GPU per eseguire la rete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In esecuzione su cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'In esecuzione su {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementiamo la rete in modo da poter gestire questa struttura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I parametri della rete sono:\n",
      "hidden.weight tensor([[-0.4137,  0.6960],\n",
      "        [ 0.2797,  0.3861]])\n",
      "hidden.bias tensor([-0.0470,  0.5276])\n",
      "output.weight tensor([[-0.2119,  0.0111]])\n",
      "output.bias tensor([0.2891])\n",
      "Questa è dunque una rete con 9 parametri.\n"
     ]
    }
   ],
   "source": [
    "class OneHidden(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(OneHidden, self).__init__()\n",
    "        self.hidden = nn.Linear(2, 2)   # connessioni dai 2 input ai 2 neuroni dello strato interno\n",
    "        self.output = nn.Linear(2, 1)   # connessioni dai 2 neuroni dello strato interno all'unico output\n",
    "\n",
    "        self.loss = nn.MSELoss()        # funzione di errore: Mean Squared Error\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=0.01) # ottimizzatore: Stochastic Gradient Descent\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def set_learning_rate(self, learning_rate):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate        \n",
    "\n",
    "    def train(self, x, y, epochs, repeat):\n",
    "        print(f'\\n*** Addestramento ***\\nepoca\\terrore (su {device})')\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()  # azzera i gradienti\n",
    "            output = self(x)            # calcola l'output\n",
    "            loss = self.loss(output, y) # calcola la funzione di errore\n",
    "            loss.backward()             # calcola i gradienti\n",
    "            self.optimizer.step()       # aggiorna i valori dei parametri\n",
    "            if (epoch+1) % repeat == 0:\n",
    "                print(f'{epoch+1}\\t{loss.item():.3f}')\n",
    "\n",
    "    def predict(self, examples, fun):\n",
    "        print('\\n*** Inferenza ***')\n",
    "        x_test = examples\n",
    "        y_test = self(x_test)           # calcola la previsione\n",
    "        y_true = self.calc_fun(fun, x_test)\n",
    "        print('x1\\tx2\\ty\\ty prev\\terrore')\n",
    "        for i in range(x_test.size(0)):\n",
    "            x1, x2 = x_test[i][0].item(), x_test[i][1].item()\n",
    "            y, y_hat = y_true[i].item(), y_test[i].item()\n",
    "            print(f'{x1:.2f}\\t{x2:.2f}\\t{y:.2f}\\t{y_hat:.2f}\\t{y - y_hat:.2f}')\n",
    "        print(f'Errore quadratico medio: {torch.mean((y_test - y_true)**2):.5f}')\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "\n",
    "    def print_parameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            print(name, param.data)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def calc_fun(self, fun, X):\n",
    "        return fun(X[:, 0], X[:, 1]).view(-1, 1).to(self.device)\n",
    "\n",
    "\n",
    "model = OneHidden(device)\n",
    "print('I parametri della rete sono:'); model.print_parameters()\n",
    "print(f\"Questa è dunque una rete con {model.count_parameters()} parametri.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruiamo il training set, negli input come un certo numero di coppie di numeri casuali e nel corrispondente output, dopo aver scelto la funzione da approssimare. Supponiamo sia la funzione massimo tra due numeri, che avevamo visto un singolo neurone non riesce ad approssimare in modo accettabile.  \n",
    "Quindi, dopo aver assegnato i valori agli iperparametri, addestriamo la rete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Addestramento ***\n",
      "epoca\terrore (su cpu)\n",
      "100\t1.281\n",
      "200\t1.281\n",
      "300\t1.281\n",
      "400\t1.281\n",
      "500\t1.281\n",
      "600\t1.281\n",
      "700\t1.281\n",
      "800\t1.281\n",
      "900\t1.281\n",
      "1000\t1.281\n"
     ]
    }
   ],
   "source": [
    "def examples(n): return (10 * torch.rand(n, 2) - 5).to(device) # genera n esempi nella forma ognuno di una coppia di numeri casuali tra -5 e 5\n",
    "num_examples = 100                      # numero di esempi per il training set\n",
    "X = examples(num_examples)              # calcola i valori degli esempi: input del training set\n",
    "def fun(x1, x2): return torch.max(x1, x2) # funzione da approssimare, in questo caso il massimo tra due numeri\n",
    "Y = model.calc_fun(fun, X)              # calcola il valore della funzione per ogni esempio: output del training set\n",
    "num_epochs = 1000                       # numero di ripetizioni del processo di addestramento\n",
    "repeat = 100                            # numero di ripetizioni dopo le quali visualizzare l'errore\n",
    "model.reset_parameters()                # reinizializza i parametri della rete\n",
    "model.set_learning_rate(0.02)           # imposta il learning rate\n",
    "model.train(X, Y, num_epochs, repeat)   # addestra la rete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "È chiaro che la presenza dello strato interno non ha migliorato le cose: nonostante un numero più elevato di ripetizioni, il processo di addestramento non è in grado di ridurre l'errore a valori accettabili.  \n",
    "La ragione dovrebbe essere chiara: stiamo cercando di approssimare una funzione non lineare con una combinazione lineare di funzioni lineari, che è a sua volta una funzione lineare.\n",
    "\n",
    "La soluzione è di introdurre un qualche genere di non linearità nella rete: la si realizza modificando la funzione calcolata dai nodi dello strato interno..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x = self.hidden(x)\n",
    "    x = torch.relu(x)                   # funzione di attivazione non lineare: ReLU\n",
    "    x = self.output(x)\n",
    "    return x\n",
    "\n",
    "OneHidden.forward = forward\n",
    "model = OneHidden(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ripetiamo il processo di addestramento su questa nuova struttura. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Addestramento ***\n",
      "epoca\terrore (su cpu)\n",
      "1000\t0.017\n",
      "2000\t0.001\n",
      "3000\t0.000\n",
      "4000\t0.000\n",
      "5000\t0.000\n",
      "6000\t0.001\n",
      "7000\t0.001\n",
      "8000\t0.000\n",
      "9000\t0.000\n",
      "10000\t0.000\n",
      "\n",
      "*** Inferenza ***\n",
      "x1\tx2\ty\ty prev\terrore\n",
      "-0.15\t-2.77\t-0.15\t-0.14\t-0.01\n",
      "1.03\t-3.97\t1.03\t1.04\t-0.01\n",
      "-0.92\t3.76\t3.76\t3.75\t0.01\n",
      "-4.43\t1.96\t1.96\t1.94\t0.01\n",
      "1.10\t-4.23\t1.10\t1.12\t-0.01\n",
      "-0.03\t0.07\t0.07\t0.07\t-0.00\n",
      "-4.82\t-3.39\t-3.39\t-3.40\t0.01\n",
      "-3.07\t-2.35\t-2.35\t-2.36\t0.00\n",
      "-1.76\t-3.66\t-1.76\t-1.76\t-0.00\n",
      "3.65\t3.21\t3.65\t3.66\t-0.01\n",
      "Errore quadratico medio: 0.00008\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000                      # numero di ripetizioni del processo di addestramento\n",
    "repeat = 1000                           # numero di ripetizioni dopo le quali visualizzare l'errore\n",
    "model.reset_parameters()                # reinizializza i parametri della rete\n",
    "model.set_learning_rate(0.02)           # imposta il learning rate\n",
    "model.train(X, Y, num_epochs, repeat)   # addestra la rete\n",
    "\n",
    "model.predict(examples(10), fun)        # inferenza dopo l'addestramento"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
