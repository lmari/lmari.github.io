{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'approssimazione di una funzione lineare mediante un singolo neurone a comportamento lineare\n",
    "\n",
    "Luca Mari, ottobre 2024  \n",
    "\n",
    "Quest'opera è distribuita con <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0\" target=\"_blank\">Licenza Creative Commons Attribuzione - Non commerciale - Condividi allo stesso modo 4.0 Internazionale</a>.  \n",
    "<img src=\"https://creativecommons.it/chapterIT/wp-content/uploads/2021/01/by-nc-sa.eu_.png\" width=\"100\">\n",
    "\n",
    "**Obiettivo**: comprendere, a partire da un esempio concreto, che una rete neurale deve includere degli elementi non lineari per poter approssimare appropriatamente anche delle semplici funzioni non lineari.  \n",
    "**Precompetenze**: basi di Python; almeno qualche idea di analisi matematica.\n",
    "\n",
    "> Per eseguire questo notebook, supponiamo con VSCode, occorre:\n",
    "> * installare un interprete Python\n",
    "> * scaricare da https://code.visualstudio.com/download e installare VSCode\n",
    "> * eseguire VSCode e attivare le estensioni per Python e Jupyter\n",
    "> * ancora in VSCode:\n",
    ">     * creare una cartella di lavoro e renderla la cartella corrente\n",
    ">     * copiare nella cartella il file di questa attività: [neuron.ipynb](neuron.ipynb)\n",
    ">     * aprire il notebook `neuron.ipynb`\n",
    ">     * creare un ambiente virtuale locale Python (Select Kernel | Python Environments | Create Python Environment | Venv, e scegliere un interprete Python):\n",
    ">     * installare il modulo Python richiesto, eseguendo dal terminale:  \n",
    ">         `pip install torch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una rete neurale è l'implementazione di una funzione parametrica $Y = f(X; \\theta)$, e può essere intesa come uno strumento di approssimazione di funzioni $F(X)$ date: attraverso un opportuno addestramento, si trovano i valori appropriati dei parametri $\\theta$ in modo che $f(X; \\theta) \\approx F(X)$.\n",
    "\n",
    "Quest'idea venne sviluppata inizialmente assumendo che i componenti elementari di una rete -- i suoi neuroni -- avessero un comportamento lineare:  \n",
    "\n",
    "![rete](neuron.drawio.svg)  \n",
    "nel caso di due input.\n",
    "\n",
    "La situazione più semplice è ovviamente quella di una rete costituita da un solo neurone. Facciamo qualche prova.\n",
    "\n",
    "Per costruire e operare sulla rete useremo `PyTorch`: importiamo perciò i moduli Python che saranno necessari e verifichiamo se è disponibile una GPU per eseguire la rete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruiamo la rete usando `PyTorch` (il codice ha un po' di dettagli tecnici, non necessariamente importanti: i commenti potrebbero essere comunque utili) e visualizziamo i valori dei suoi parametri, che inizialmente sono casuali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I parametri della rete sono:\n",
      "neuron.weight tensor([[ 0.1170, -0.1727]])\n",
      "neuron.bias tensor([0.3492])\n"
     ]
    }
   ],
   "source": [
    "class OneNeuron(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(OneNeuron, self).__init__()\n",
    "        self.neuron = nn.Linear(2, 1)\n",
    "\n",
    "        self.loss = nn.MSELoss()        # funzione di errore: Mean Squared Error\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=0.01) # ottimizzatore: Stochastic Gradient Descent\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.neuron(x)\n",
    "        return x\n",
    "\n",
    "    def set_learning_rate(self, learning_rate):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate        \n",
    "\n",
    "    def train(self, x, y, epochs, repeat):\n",
    "        print(f'\\n*** Addestramento ***\\nepoca\\terrore (su {device})')\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()  # azzera i gradienti\n",
    "            output = self(x)            # calcola l'output\n",
    "            loss = self.loss(output, y) # calcola la funzione di errore\n",
    "            loss.backward()             # calcola i gradienti\n",
    "            self.optimizer.step()       # aggiorna i valori dei parametri\n",
    "            if (epoch+1) % repeat == 0:\n",
    "                print(f'{epoch+1}\\t{loss.item():.3f}')\n",
    "\n",
    "    def predict(self, examples, fun):\n",
    "        print('\\n*** Inferenza ***')\n",
    "        x_test = examples\n",
    "        y_test = self(x_test)           # calcola la previsione\n",
    "        y_true = self.calc_fun(fun, x_test)\n",
    "        print('x1\\tx2\\ty\\ty prev\\terrore')\n",
    "        for i in range(x_test.size(0)):\n",
    "            x1, x2 = x_test[i][0].item(), x_test[i][1].item()\n",
    "            y, y_hat = y_true[i].item(), y_test[i].item()\n",
    "            print(f'{x1:.2f}\\t{x2:.2f}\\t{y:.2f}\\t{y_hat:.2f}\\t{y - y_hat:.2f}')\n",
    "        print(f'Errore quadratico medio: {torch.mean((y_test - y_true)**2):.5f}')\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "\n",
    "    def print_parameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            print(name, param.data)\n",
    "\n",
    "    def calc_fun(self, fun, X):\n",
    "        return fun(X[:, 0], X[:, 1]).view(-1, 1).to(self.device)\n",
    "\n",
    "\n",
    "model = OneNeuron(device)\n",
    "print('I parametri della rete sono:'); model.print_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruiamo il training set, prima di tutto negli input (_features_, _covariates_) come un certo numero di coppie di numeri casuali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examples(n): return (10 * torch.rand(n, 2) - 5).to(device) # genera n esempi nella forma ognuno di una coppia di numeri casuali tra -5 e 5\n",
    "num_examples = 100                      # numero di esempi per il training set\n",
    "X = examples(num_examples)              # calcola i valori degli esempi: input del training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scegliamo la funzione, dunque a due argomenti, da approssimare. Essendo un caso di _supervised learning_, calcoliamo la funzione per tutte le coppie del training set e aggiungiamo il risultato al training set stesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'esempio di una tripla nel training set: x1=1.591, x2=-4.751, y=-1.580\n"
     ]
    }
   ],
   "source": [
    "def fun(x1, x2): return (x1 + x2) / 2   # funzione da approssimare, in questo caso la media tra due numeri\n",
    "Y = model.calc_fun(fun, X)              # calcola il valore della funzione per ogni esempio: output del training set\n",
    "print(f\"L'esempio di una tripla nel training set: x1={X[0, 0].item():.3f}, x2={X[0, 1].item():.3f}, y={Y[0, 0].item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Già ora possiamo mettere in funzione la rete, su un certo numero di esempi che costituiscono dunque un test set, ma ovviamente il risultato non sarà in alcun modo accurato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Inferenza ***\n",
      "x1\tx2\ty\ty prev\terrore\n",
      "-3.10\t3.60\t0.25\t-0.63\t0.89\n",
      "3.12\t0.50\t1.81\t0.63\t1.19\n",
      "3.73\t1.52\t2.62\t0.52\t2.10\n",
      "2.33\t-1.73\t0.30\t0.92\t-0.62\n",
      "-1.48\t-3.13\t-2.30\t0.72\t-3.02\n",
      "2.02\t3.60\t2.81\t-0.04\t2.85\n",
      "3.96\t-2.67\t0.64\t1.27\t-0.63\n",
      "1.60\t4.63\t3.11\t-0.26\t3.38\n",
      "2.45\t3.81\t3.13\t-0.02\t3.15\n",
      "-3.87\t1.27\t-1.30\t-0.32\t-0.97\n",
      "Errore quadratico medio: 4.68676\n"
     ]
    }
   ],
   "source": [
    "model.predict(examples(10), fun)        # inferenza prima dell'addestramento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addestriamo allora la rete, dopo aver assegnato valori opportuni ai due iperparametri fondamentali:  \n",
    "-- il numero di volte in cui il processo di addestramento viene ripetuto, e  \n",
    "-- la velocità di apprendimento (_learning rate_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Addestramento ***\n",
      "epoca\terrore (su cpu)\n",
      "10\t0.006\n",
      "20\t0.000\n",
      "30\t0.000\n",
      "40\t0.000\n",
      "50\t0.000\n",
      "60\t0.000\n",
      "70\t0.000\n",
      "80\t0.000\n",
      "90\t0.000\n",
      "100\t0.000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100                        # numero di ripetizioni del processo di addestramento\n",
    "repeat = 10                             # numero di ripetizioni dopo le quali visualizzare l'errore\n",
    "model.reset_parameters()                # reinizializza i parametri della rete\n",
    "model.set_learning_rate(0.02)           # imposta il learning rate\n",
    "model.train(X, Y, num_epochs, repeat)   # addestra la rete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mettiamo in funzione la rete su un nuovo test set: se l'addestramento ha avuto successo, si dovrebbe ottenere un piccolo errore quadratico medio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Inferenza ***\n",
      "x1\tx2\ty\ty prev\terrore\n",
      "1.08\t-0.75\t0.16\t0.16\t0.00\n",
      "-4.01\t3.09\t-0.46\t-0.46\t0.00\n",
      "-0.60\t0.50\t-0.05\t-0.05\t0.00\n",
      "3.19\t-3.25\t-0.03\t-0.03\t0.00\n",
      "-0.02\t-2.84\t-1.43\t-1.43\t0.00\n",
      "-3.96\t2.43\t-0.76\t-0.76\t0.00\n",
      "1.15\t2.75\t1.95\t1.95\t0.00\n",
      "0.76\t-0.11\t0.32\t0.32\t0.00\n",
      "1.96\t-2.01\t-0.03\t-0.03\t0.00\n",
      "-2.10\t1.57\t-0.27\t-0.27\t0.00\n",
      "Errore quadratico medio: 0.00000\n"
     ]
    }
   ],
   "source": [
    "model.predict(examples(10), fun)        # inferenza dopo l'addestramento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizziamo i valori dei parametri della rete: se l'addestramento ha avuto successo, dovrebbero essere vicini ai valori attesi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuron.weight tensor([[0.5000, 0.5001]])\n",
      "neuron.bias tensor([-0.0007])\n"
     ]
    }
   ],
   "source": [
    "model.print_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La struttura della rete è così semplice che possiamo ripetere l'intero processo senza ricorrere a `PyTorch`, per mostrare così in modo esplicito la logica della procedura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Addestramento ***\n",
      "epoca\terrore\tk0\tk1\tk2\n",
      "10\t1.231\t-1.109\t0.573\t0.366\n",
      "20\t0.863\t-0.901\t0.518\t0.490\n",
      "30\t0.609\t-0.738\t0.494\t0.538\n",
      "40\t0.310\t-0.604\t0.503\t0.553\n",
      "50\t0.285\t-0.499\t0.518\t0.527\n",
      "60\t0.182\t-0.415\t0.491\t0.542\n",
      "70\t0.112\t-0.345\t0.509\t0.534\n",
      "80\t0.082\t-0.283\t0.504\t0.514\n",
      "90\t0.048\t-0.233\t0.505\t0.523\n",
      "100\t0.044\t-0.192\t0.491\t0.506\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100                        # numero di ripetizioni del processo di addestramento\n",
    "repeat = 10                             # numero di ripetizioni dopo le quali visualizzare l'errore\n",
    "learning_rate = 0.01                    # learning rate\n",
    "minibatch_size = 10                     # dimensione del minibatch: numero di esempi estratti dal training set per ogni epoca\n",
    "\n",
    "k0, k1, k2 = torch.randn(3)             # valori casuali di inizializzazione dei parametri \n",
    "\n",
    "print(f'\\n*** Addestramento ***\\nepoca\\terrore\\tk0\\tk1\\tk2')\n",
    "for i in range(num_epochs):\n",
    "    indexes = torch.randperm(X.size(0))[:minibatch_size]            # seleziona in modo casuale gli indici del minibatch\n",
    "    X1 = X[indexes, 0]                                              # estrai dal training set gli argomenti della funzione\n",
    "    X2 = X[indexes, 1]\n",
    "    Y_prev = k0 + k1 * X1 + k2 * X2                                 # calcola la previsione\n",
    "    Y_true = Y[indexes, 0]                                          # estrai dal training set il valore della funzione\n",
    "    loss = torch.mean((Y[indexes, 0] - Y_prev)**2)                  # calcola la funzione di errore (errore quadratico medio)\n",
    "    k0 -= learning_rate * 2 * torch.mean(Y_prev - Y_true)           # calcola le derivate parziali della funzione di errore...\n",
    "    k1 -= learning_rate * 2 * torch.mean((Y_prev - Y_true) * X1)    # ... e aggiorna i valori dei parametri...\n",
    "    k2 -= learning_rate * 2 * torch.mean((Y_prev - Y_true) * X2)    # ... dunque \"scendendo lungo il gradiente\"\n",
    "    if (i+1) % repeat == 0:\n",
    "        print(f'{i+1}\\t{loss.item():.3f}\\t{k0:.3f}\\t{k1:.3f}\\t{k2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quella che segue è invece un'implementazione semplificata di un algoritmo genetico, per risolvere lo stesso problema di ottimizzazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Addestramento ***\n",
      "epoca\terrore\tk0\tk1\tk2\n",
      "10\t0.091\t0.192\t0.556\t0.433\n",
      "20\t0.190\t0.139\t0.556\t0.314\n",
      "30\t0.123\t-0.687\t0.507\t0.544\n",
      "40\t0.077\t-0.199\t0.462\t0.522\n",
      "50\t0.058\t-0.199\t0.462\t0.475\n",
      "60\t0.007\t-0.052\t0.513\t0.518\n",
      "70\t0.004\t-0.052\t0.513\t0.466\n",
      "80\t0.026\t-0.078\t0.513\t0.569\n",
      "90\t0.019\t0.142\t0.506\t0.477\n",
      "100\t0.016\t-0.026\t0.513\t0.446\n"
     ]
    }
   ],
   "source": [
    "num_individuals = 100                   # numero di individui della popolazione in evoluzione\n",
    "num_survivors = 50                      # numero di individui che in ogni epoca sopravvive\n",
    "num_mutations = 5                       # numero di individui che in ogni epoca subisce una mutazione \n",
    "width_mutations = .1                    # ampiezza (deviazione standard) delle mutazioni\n",
    "minibatch_size = 10                     # dimensione del minibatch: numero di esempi estratti dal training set per ogni epoca\n",
    "num_epochs = 100                        # numero di ripetizioni del processo di addestramento\n",
    "repeat = 10                             # numero di ripetizioni dopo le quali visualizzare l'errore\n",
    "\n",
    "k = torch.randn(num_individuals, 3)\n",
    "\n",
    "print(f'\\n*** Addestramento ***\\nepoca\\terrore\\tk0\\tk1\\tk2')\n",
    "for i in range(num_epochs):\n",
    "    indexes = torch.randperm(X.size(0))[:minibatch_size]            # seleziona in modo casuale gli indici del minibatch\n",
    "    X1 = X[indexes, 0].view(-1, 1).T                                # estrai dal training set gli argomenti della funzione\n",
    "    X2 = X[indexes, 1].view(-1, 1).T\n",
    "    Y_true = Y[indexes, 0].view(-1, 1).T                            # estrai dal training set il valore della funzione\n",
    "    Y_prev = k[:,0].view(-1, 1) + k[:,1].view(-1, 1) * X1 + k[:,2].view(-1, 1) * X2 # calcola la previsione\n",
    "\n",
    "    loss = torch.mean((Y[indexes, 0] - Y_prev)**2, dim=1)           # calcola la funzione di errore per ogni individuo\n",
    "    sorted_indexes = torch.argsort(loss, descending=False)          # ottieni gli indici degli individui ordinati in base all'errore\n",
    "    k = k[sorted_indexes][:num_survivors]                           # ordina gli individui per fitness e seleziona i migliori\n",
    "\n",
    "    m0 = torch.randint(num_survivors, (num_mutations, 1)).view(-1)  # seleziona casualmente gli indici degli individui da mutare\n",
    "    m1 = torch.randint(3, (num_mutations, 1)).view(-1)\n",
    "    k[m0, m1] += torch.randn(num_mutations) * width_mutations       # introduci una mutazione negli individui selezionati\n",
    "\n",
    "    k = torch.cat((k, torch.randn(num_individuals - num_survivors, 3)), 0) # reintegra la popolazione con nuovi individui casuali\n",
    "\n",
    "    if (i+1) % repeat == 0:\n",
    "        best = k[sorted_indexes][0]\n",
    "        print(f'{i+1}\\t{loss[sorted_indexes][0].item():.3f}\\t{best[0].item():.3f}\\t{best[1].item():.3f}\\t{best[2].item():.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tornando ora a usare `PyTorch`: d'altra parte, è evidente che un singolo neurone a comportamento lineare può approssimare efficacemente solo funzioni molto semplici. Anche aumentando il numero di esempi e di ripetizioni del processo di addestramento, per esempio non è in grado di approssimare in modo accettabile la funzione massimo tra due numeri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Addestramento ***\n",
      "epoca\terrore (su cuda)\n",
      "100\t1.429\n",
      "200\t1.402\n",
      "300\t1.402\n",
      "400\t1.402\n",
      "500\t1.402\n",
      "600\t1.402\n",
      "700\t1.402\n",
      "800\t1.402\n",
      "900\t1.402\n",
      "1000\t1.402\n",
      "\n",
      "*** Inferenza ***\n",
      "x1\tx2\ty\ty prev\terrore\n",
      "-2.97\t-2.33\t-2.33\t-1.08\t-1.25\n",
      "2.32\t-2.67\t2.32\t1.44\t0.88\n",
      "-4.40\t-1.10\t-1.10\t-1.18\t0.08\n",
      "-2.38\t0.51\t0.51\t0.67\t-0.15\n",
      "-4.92\t2.27\t2.27\t0.26\t2.01\n",
      "-2.52\t3.99\t3.99\t2.36\t1.63\n",
      "0.35\t-0.05\t0.35\t1.77\t-1.42\n",
      "-4.51\t4.43\t4.43\t1.57\t2.86\n",
      "0.71\t3.10\t3.10\t3.55\t-0.44\n",
      "0.94\t-2.44\t0.94\t0.85\t0.08\n",
      "Errore quadratico medio: 1.94872\n"
     ]
    }
   ],
   "source": [
    "num_examples = 1000                     # numero di esempi per il training set\n",
    "X = examples(num_examples)              # input del training set\n",
    "def fun(x1, x2): return torch.max(x1, x2) # funzione da approssimare, in questo caso il massimo tra due numeri\n",
    "Y = model.calc_fun(fun, X)              # calcola il valore della funzione per ogni esempio: output del training set\n",
    "num_epochs = 1000                       # numero di ripetizioni del processo di addestramento\n",
    "repeat = 100                            # numero di ripetizioni dopo le quali visualizzare l'errore\n",
    "model.reset_parameters()                # reinizializza i parametri della rete\n",
    "model.set_learning_rate(0.01)           # imposta il learning rate\n",
    "model.train(X, Y, num_epochs, repeat)   # addestra la rete\n",
    "model.predict(examples(10), fun)        # metti in funzione la rete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ottenere approssimazioni accettabili occorre dunque costruire una rete più complessa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
