{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9ZhCh9jQdVX"
   },
   "source": [
    "# Qualche esempio di uso dei transformer \n",
    "\n",
    "Luca Mari, settembre 2024  \n",
    "\n",
    "Quest'opera è distribuita con <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0\" target=\"_blank\">Licenza Creative Commons Attribuzione - Non commerciale - Condividi allo stesso modo 4.0 Internazionale</a>.  \n",
    "<img src=\"https://creativecommons.it/chapterIT/wp-content/uploads/2021/01/by-nc-sa.eu_.png\" width=\"100\">\n",
    "\n",
    "**Obiettivo**: essere informati di alcune possibili modalità di uso \"di alto livello\" dei trasformer per applicazioni con testi in lingua italiana e della libreria Python `transformers` di Hugging Face.  \n",
    "**Precompetenze**: basi di Python.\n",
    "\n",
    "> Per eseguire questo notebook con VSCode sul proprio calcolatore, occorre:\n",
    "> * installare un interprete Python\n",
    "> * scaricare da https://code.visualstudio.com/download e installare VSCode\n",
    "> * eseguire VSCode e attivare le estensioni per Python e Jupyter\n",
    "> * ancora in VSCode:\n",
    ">     * creare una cartella di lavoro e renderla la cartella corrente\n",
    ">     * copiare nella cartella il file di questa attività: [transformers.ipynb](transformers.ipynb)\n",
    ">     * aprire il notebook `transformers.ipynb`\n",
    ">     * creare un ambiente virtuale locale Python (Select Kernel | Python Environments | Create Python Environment | Venv, e scegliere un interprete Python):\n",
    ">     * installare i moduli Python richiesti, eseguendo dal terminale:  \n",
    ">         `pip install transformer torch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ch1W02NLQdVZ"
   },
   "source": [
    "Sappiamo che con sistemi come `ChatGPT` possiamo dialogare liberamente e in molte lingue. Grazie al modulo Python `transformers` di Hugging Face (https://huggingface.co/docs/transformers) e ai tanti modelli accessibili e scaricabili liberamente da Hugging Face stesso, con poche linee di codice Python è possibile costruire applicazioni con funzionalità specifiche, anche in grado di trattare testi in lingua italiana con una qualità spesso accettabile.\n",
    "\n",
    "I modelli in questione sono spesso di dimensioni relativamente piccole, e perciò possono essere eseguiti in locale. Anche se condividono la stessa architettura di base -- perché sono tutti transformer --, sono tipicamente addestrati per applicazioni specifiche.\n",
    "\n",
    "Nel seguito mostriamo alcuni esempi, per ognuno dei quali usiamo un modello appropriato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa importiamo i moduli Python che useremo e specifichiamo che, per semplicità, faremo eseguire il codice sempre in CPU anche nel caso una GPU sia disponibile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from pprint import pprint\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sentiment analysis\n",
    "https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': '1 star', 'score': 0.011834262870252132},\n",
      "  {'label': '2 stars', 'score': 0.05826163291931152},\n",
      "  {'label': '3 stars', 'score': 0.5082424879074097},\n",
      "  {'label': '4 stars', 'score': 0.32383590936660767},\n",
      "  {'label': '5 stars', 'score': 0.09782570600509644}]]\n"
     ]
    }
   ],
   "source": [
    "model = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, device=device)\n",
    "res = classifier(\n",
    "    \"Ho trovato abbastanza interessante questa lezione sull'intelligenza artificiale\",\n",
    "    return_all_scores=True\n",
    ")\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Zero-shot-classification\n",
    "https://huggingface.co/Jiva/xlm-roberta-large-it-mnli (560M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': ['tecnologia',\n",
      "            'storia',\n",
      "            'geografia',\n",
      "            'politica',\n",
      "            'economia',\n",
      "            'sport'],\n",
      " 'scores': [0.40379834175109863,\n",
      "            0.23238512873649597,\n",
      "            0.10381027311086655,\n",
      "            0.09817719459533691,\n",
      "            0.08147850632667542,\n",
      "            0.0803506076335907],\n",
      " 'sequence': 'Ho trovato abbastanza interessante questa lezione su Alan Turing'}\n"
     ]
    }
   ],
   "source": [
    "model = 'Jiva/xlm-roberta-large-it-mnli'\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model, device=device)\n",
    "res= classifier(\n",
    "    \"Ho trovato abbastanza interessante questa lezione su Alan Turing\",\n",
    "    candidate_labels=[\"economia\", \"geografia\", \"politica\", \"sport\", \"storia\", \"tecnologia\"]\n",
    ")\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Named Entity Recognition (NER)\n",
    "https://huggingface.co/osiria/deberta-base-italian-uncased-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'end': 42,\n",
      "  'entity_group': 'PER',\n",
      "  'score': np.float32(0.96193117),\n",
      "  'start': 32,\n",
      "  'word': 'Luca Mari'},\n",
      " {'end': 78,\n",
      "  'entity_group': 'ORG',\n",
      "  'score': np.float32(0.93260586),\n",
      "  'start': 66,\n",
      "  'word': 'Politecnico'},\n",
      " {'end': 88,\n",
      "  'entity_group': 'LOC',\n",
      "  'score': np.float32(0.91371405),\n",
      "  'start': 81,\n",
      "  'word': 'Milano'}]\n"
     ]
    }
   ],
   "source": [
    "model = 'osiria/deberta-base-italian-uncased-ner'\n",
    "ner = pipeline(\"ner\", model=model, device=device, grouped_entities=True)\n",
    "res = ner(\"Questa lezione è stata tenuta da Luca Mari per un corso master del Politecnico di Milano\")\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fill mask\n",
    "https://huggingface.co/Musixmatch/umberto-wikipedia-uncased-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Musixmatch/umberto-wikipedia-uncased-v1 were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.7675500512123108,\n",
      "  'sequence': 'n questa lezione stiamo discutendo di intelligenza artificiale.',\n",
      "  'token': 12582,\n",
      "  'token_str': 'intelligenza'},\n",
      " {'score': 0.043854448944330215,\n",
      "  'sequence': 'n questa lezione stiamo discutendo di lingua artificiale.',\n",
      "  'token': 1476,\n",
      "  'token_str': 'lingua'},\n",
      " {'score': 0.010562906041741371,\n",
      "  'sequence': 'n questa lezione stiamo discutendo di selezione artificiale.',\n",
      "  'token': 7268,\n",
      "  'token_str': 'selezione'}]\n"
     ]
    }
   ],
   "source": [
    "model = 'Musixmatch/umberto-wikipedia-uncased-v1'\n",
    "unmasker = pipeline(\"fill-mask\", model=model, device=device)\n",
    "res = unmasker(\n",
    "    \"In questa lezione stiamo discutendo di <mask> artificiale.\",\n",
    "    top_k=3\n",
    ")\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Question answering\n",
    "https://huggingface.co/osiria/deberta-italian-question-answering (124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9232677817344666,\n",
       " 'start': 53,\n",
       " 'end': 75,\n",
       " 'answer': ' Politecnico di Milano'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = 'osiria/deberta-italian-question-answering'\n",
    "question_answerer = pipeline(\"question-answering\", model=model, device=device)\n",
    "question_answerer(\n",
    "    context=\"Stiamo tenendo questa lezione per un corso master del Politecnico di Milano\",\n",
    "    question=\"Per quale organizzazione si sta tenendo questa lezione?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Text generation (completamento)\n",
    "https://huggingface.co/microsoft/Phi-3.5-mini-instruct (3.82B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8520c6e14ebf4cbf8264476d46d5495a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"In questa lezione sull'intelligenza artificiale, stiamo \"\n",
      "                    'trattando di un modello di apprendimento automatico '\n",
      "                    'chiamato rete neurale. Una rete neurale è una struttura '\n",
      "                    'computazionale ispir'}]\n"
     ]
    }
   ],
   "source": [
    "model = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "generator = pipeline(\"text-generation\", model=model, device=device)\n",
    "res = generator(\n",
    "    \"In questa lezione sull'intelligenza artificiale, stiamo trattando di\",\n",
    "    max_length=50,\n",
    "    truncation=True\n",
    ")\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Text generation (Q&A)\n",
    "https://huggingface.co/microsoft/Phi-3.5-mini-instruct (3.82B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f462c6a138c4487f9cbb920a55eb5595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': [{'content': 'Sei un esperto che risponde in italiano. e '\n",
      "                                 'sempre in modo amichevole.',\n",
      "                      'role': 'system'},\n",
      "                     {'content': '\\n'\n",
      "                                 '                Fai una sintesi in due frasi '\n",
      "                                 'del testo che segue?\\n'\n",
      "                                 '                Il metodo scientifico ci ha '\n",
      "                                 'insegnato a fare previsioni sulla base (1) '\n",
      "                                 'di modelli dei fenomeni e (2) di dati: un '\n",
      "                                 'modello ci dice che se il fenomeno è in una '\n",
      "                                 'certa condizione, osserveremo certi effetti, '\n",
      "                                 'e i dati ci dicono che siamo in una certa '\n",
      "                                 'condizione; se tutto ciò è sufficientemente '\n",
      "                                 'accurato, possiamo supporre che osserveremo '\n",
      "                                 'proprio gli effetti predetti dal modello in '\n",
      "                                 'base ai dati.\\n'\n",
      "                                 '                Si tratta di una versione '\n",
      "                                 'razionalizzata di ragionamenti che tutti '\n",
      "                                 'noi, anche senza rendercene conto, attuiamo '\n",
      "                                 \"abitualmente: dall'idea, un modello appunto, \"\n",
      "                                 'che ci siamo fatti di come la nostra '\n",
      "                                 'automobile si comporta quando freniamo e dai '\n",
      "                                 'dati che otteniamo osservando intorno a noi '\n",
      "                                 'mentre guidiamo, concludiamo se, quando e '\n",
      "                                 'quanto intensamente dobbiamo schiacciare il '\n",
      "                                 'freno.\\n'\n",
      "                                 '                Ma in questo momento non '\n",
      "                                 'abbiamo modelli affidabili di quello che sta '\n",
      "                                 'succedendo a proposito dei sistemi '\n",
      "                                 'cosiddetti di intelligenza artificiale '\n",
      "                                 'generativa, e quindi questa strategia non è '\n",
      "                                 'applicabile.\\n'\n",
      "                                 '                Quando non abbiamo un '\n",
      "                                 'modello a cui affidarci, a volte ricorriamo '\n",
      "                                 'a una strategia di riserva: nel passato, in '\n",
      "                                 'una certa condizione si sono prodotti certi '\n",
      "                                 'effetti; se ora i dati ci dicono che siamo '\n",
      "                                 'in una condizione simile, per analogia '\n",
      "                                 'ipotizziamo che si produrranno effetti '\n",
      "                                 'simili.\\n'\n",
      "                                 '                Ragionando per analogia '\n",
      "                                 'possiamo essere creativi, se troviamo nuove '\n",
      "                                 'connessioni tra le entità a cui ci stiamo '\n",
      "                                 'interessando, ma paradossalmente anche '\n",
      "                                 'inerti di fronte ai cambiamenti, se ci '\n",
      "                                 'ostiniamo a interpretare le novità alla luce '\n",
      "                                 'di quello che già sappiamo.\\n'\n",
      "                                 '                Ma anche questa strategia '\n",
      "                                 'analogica non funziona oggi a proposito dei '\n",
      "                                 'sistemi di intelligenza artificiale '\n",
      "                                 'generativa: sia perché stiamo vivendo una '\n",
      "                                 'condizione decisamente nuova, e quindi non '\n",
      "                                 'sappiamo bene con cosa del passato '\n",
      "                                 'confrontarla, sia perché le analogie sono '\n",
      "                                 'valide quando i cambiamenti sono lineari, e '\n",
      "                                 'invece i cambiamenti che stiamo vivendo in '\n",
      "                                 'questi mesi sembrano proprio esponenziali.\\n'\n",
      "                                 '                Insomma, se è spesso '\n",
      "                                 'difficile fare previsioni affidabili senza '\n",
      "                                 'modelli e dati affidabili, nella situazione '\n",
      "                                 'in cui siamo potremmo ammettere onestamente '\n",
      "                                 'che le nostre previsioni non hanno davvero '\n",
      "                                 'nulla di affidabile.\\n'\n",
      "                                 '            ',\n",
      "                      'role': 'user'},\n",
      "                     {'content': ' Il metodo scientifico basato su modelli e '\n",
      "                                 'dati è inefficace per prevedere i sistemi di '\n",
      "                                 'intelligenza artificiale generativa a causa '\n",
      "                                 'della loro natura nuova e dei cambiamenti '\n",
      "                                 'esponenziali. Senza modelli affidabili e '\n",
      "                                 'dati affidabili, le nostre previsioni '\n",
      "                                 'riguardo a questi sistemi sono prive di '\n",
      "                                 'affidabilità.',\n",
      "                      'role': 'assistant'}]}]\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/microsoft/Phi-3.5-mini-instruct (3.82B)\n",
    "model = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "generator = pipeline(\"text-generation\", model=model, device=device)\n",
    "res = generator(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Sei un esperto che risponde in italiano. e sempre in modo amichevole.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"\n",
    "                Fai una sintesi in due frasi del testo che segue?\n",
    "                Il metodo scientifico ci ha insegnato a fare previsioni sulla base (1) di modelli dei fenomeni e (2) di dati: un modello ci dice che se il fenomeno è in una certa condizione, osserveremo certi effetti, e i dati ci dicono che siamo in una certa condizione; se tutto ciò è sufficientemente accurato, possiamo supporre che osserveremo proprio gli effetti predetti dal modello in base ai dati.\n",
    "                Si tratta di una versione razionalizzata di ragionamenti che tutti noi, anche senza rendercene conto, attuiamo abitualmente: dall'idea, un modello appunto, che ci siamo fatti di come la nostra automobile si comporta quando freniamo e dai dati che otteniamo osservando intorno a noi mentre guidiamo, concludiamo se, quando e quanto intensamente dobbiamo schiacciare il freno.\n",
    "                Ma in questo momento non abbiamo modelli affidabili di quello che sta succedendo a proposito dei sistemi cosiddetti di intelligenza artificiale generativa, e quindi questa strategia non è applicabile.\n",
    "                Quando non abbiamo un modello a cui affidarci, a volte ricorriamo a una strategia di riserva: nel passato, in una certa condizione si sono prodotti certi effetti; se ora i dati ci dicono che siamo in una condizione simile, per analogia ipotizziamo che si produrranno effetti simili.\n",
    "                Ragionando per analogia possiamo essere creativi, se troviamo nuove connessioni tra le entità a cui ci stiamo interessando, ma paradossalmente anche inerti di fronte ai cambiamenti, se ci ostiniamo a interpretare le novità alla luce di quello che già sappiamo.\n",
    "                Ma anche questa strategia analogica non funziona oggi a proposito dei sistemi di intelligenza artificiale generativa: sia perché stiamo vivendo una condizione decisamente nuova, e quindi non sappiamo bene con cosa del passato confrontarla, sia perché le analogie sono valide quando i cambiamenti sono lineari, e invece i cambiamenti che stiamo vivendo in questi mesi sembrano proprio esponenziali.\n",
    "                Insomma, se è spesso difficile fare previsioni affidabili senza modelli e dati affidabili, nella situazione in cui siamo potremmo ammettere onestamente che le nostre previsioni non hanno davvero nulla di affidabile.\n",
    "            \"\"\"\n",
    "        }\n",
    "    ],\n",
    "    max_new_tokens=128\n",
    ")\n",
    "pprint(res)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
