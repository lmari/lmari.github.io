{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un'esplorazione della tokenizzazione\n",
    "\n",
    "Luca Mari, settembre 2024  \n",
    "\n",
    "Quest'opera è distribuita con <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0\" target=\"_blank\">Licenza Creative Commons Attribuzione - Non commerciale - Condividi allo stesso modo 4.0 Internazionale</a>.  \n",
    "<img src=\"https://creativecommons.it/chapterIT/wp-content/uploads/2021/01/by-nc-sa.eu_.png\" width=\"100\">\n",
    "\n",
    "**Obiettivo**: comprendere la logica della \"tokenizzazione\", il processo con cui un testo viene trasformato in una successione di elementi linguistici elementari (\"token\").  \n",
    "**Precompetenze**: basi di Python.\n",
    "\n",
    "> Per eseguire questo notebook, supponiamo con VSCode, occorre:\n",
    "> * installare un interprete Python\n",
    "> * scaricare da https://code.visualstudio.com/download e installare VSCode\n",
    "> * eseguire VSCode e attivare le estensioni per Python e Jupyter\n",
    "> * ancora in VSCode:\n",
    ">     * creare una cartella di lavoro e renderla la cartella corrente\n",
    ">     * copiare nella cartella i file di questa attività: [tokenize.ipynb](tokenize.ipynb), [tokenizeutils.py](tokenizeutils.py)]\n",
    ">     * aprire il notebook `tokenize.ipynb`\n",
    ">     * creare un ambiente virtuale locale Python (Select Kernel | Python Environments | Create Python Environment | Venv, e scegliere un interprete Python):\n",
    ">     * installare il modulo Python richiesto, eseguendo dal terminale:  \n",
    ">         `pip install torch transformers multimethod colorama python-docx`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una prima elaborazione di un testo dato -- una frase, un paragrafo, un documento, ... -- è spesso la sua trasformazione in una successione di elementi linguistici elementari (\"token\"), tratti da un vocabolario dato.  \n",
    "Per mostrare un esempio di questo processo di tokenizzazione, per prima cosa importiamo il modulo che contiene le funzioni per consentire un accesso \"di alto livello\" al modello pre-addestrato che opererà come tokenizzatore, usando in questo caso una versione pre-addestrata e fine tuned, su testi in italiano, di `BERT`, che è un transformer \"open\" (https://it.wikipedia.org/wiki/BERT) ed eseguibile anche localmente (alla prima esecuzione sarà dunque necessario attendere che il modello sia scaricato dal sito di Hugging Face: è un file di circa 400 MB che viene copiato nella cartella HFdata della propria cartella personale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il tokenizzatore ha un vocabolario di 31102 token che riconosce.\n"
     ]
    }
   ],
   "source": [
    "from tokenizeutils import Model, colorize\n",
    "\n",
    "model = Model('dbmdz/bert-base-italian-xxl-cased', True)\n",
    "\n",
    "print(f\"Il tokenizzatore ha un vocabolario di {model.vocab_size} token che riconosce.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data una frase, chiediamo al tokenizzatore di trasformarla nella lista dei suoi token, ognuno dei quali corrispondente a un identificatore numerico univoco nel vocabolario.  \n",
    "Può accadere ovviamente che una parola non sia presente nel vocabolario, cioè non sia riconosciuta come un token: in tal caso il tokenizzatore la separa in componenti, ognuno dei quali sia invece un token (per rendere evidente questa separazione, il tokenizzatore aggiunge la stringa \"##\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'sistemi', 'di', 'intelligenza', 'artificiale', 'generati', '##va', 'stanno', 'evol', '##vendo', 'rapidamente', '.']\n",
      "[184, 2839, 120, 21486, 13435, 29518, 187, 2799, 8599, 4802, 5798, 697]\n"
     ]
    }
   ],
   "source": [
    "frase = \"I sistemi di intelligenza artificiale generativa stanno evolvendo rapidamente.\"\n",
    "tokens = model.tokenizer.tokenize(frase)\n",
    "print(tokens)\n",
    "token_ids = model.tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternando i colori, il risultato della tokenizzazione è forse ancora più chiaro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m I\u001b[34m sistemi\u001b[31m di\u001b[34m intelligenza\u001b[31m artificiale\u001b[34m generati\u001b[31m va\u001b[34m stanno\u001b[31m evol\u001b[34m vendo\u001b[31m rapidamente\u001b[34m .\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(colorize(tokens, clean=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poiché la funzione che associa un identificatore numerico a ogni token è invertibile, è sempre possibile ricostruire la frase di partenza dalla successione degli identificatori numerici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I sistemi di intelligenza artificiale generativa stanno evolvendo rapidamente.\n"
     ]
    }
   ],
   "source": [
    "print(model.tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per curiosità, possiamo facilmente esplorare il contenuto del vocabolario, elencando alcuni dei token che contiene: è chiaro che non c'è una logica chiara nell'ordine con cui i token compaiono. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dichiarato opportuno ordini arrivo lasci SP Può sogno bellezza ##OD\n",
      "##scritto Zo vicin impegno lavorato conformemente ##ttorno Ancora faceva esplo\n",
      "magari Max ##tria produttore ricordare ##mentato dottor Slo quota resti\n",
      "presentazione 92 é ##nno privata UN accol strade Air collegamento\n",
      "##isca Antonio Dan arma offerta spettacolo ##ustralia ##uori occidentale ##74\n",
      "Ricor My fla ##-200 custo Belgio ##.12. Barcellona ##autorità motori\n",
      "assicurare ##esercizio ##bio sport ##95 bol frattempo deten ##cenze ##TER\n",
      "##uogo disciplina numeri ##teriormente ##gelo ottimo registrato ##esame ricord selezione\n",
      "##inione consegui ##ds ##tentri conosco tradizionale trasferimento qualifica interni ##derci\n",
      "rea proposto Natura ##monian adulti World ##glione ##onale ##ffo ##Com\n"
     ]
    }
   ],
   "source": [
    "for i in range(6100, 6200):\n",
    "    print(model.tokenizer.decode([i]), end=('\\n' if i % 10 == 9 else ' '))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
